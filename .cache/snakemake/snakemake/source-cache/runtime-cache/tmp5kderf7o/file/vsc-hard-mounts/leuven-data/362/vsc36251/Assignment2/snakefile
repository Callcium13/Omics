
vsc_number = "36251"  # VSC number for filtering id's

# get the list of names from the genomes folder
sample_names, = glob_wildcards("/staging/leuven/stg_00079/teaching/1000genomes/{sample}.fq.gz")
#filter that list based on the last digit
sample_names = [id for id in sample_names if id[6] == vsc_number[-1]]

#Additionally, to lighten the load further, we extracted fastq reads of just chromosome 21 for you to map
BWA_DB_chr21 = '/lustre1/project/stg_00079/teaching/hg38_21/chr21.fa'

#get the jar and data for SNPeff
snpeff_jar = "/staging/leuven/stg_00079/teaching/I0U19a_conda_2024/share/snpeff-5.2-0/snpEff.jar"
snpeff_genome = 'hg38'
snpeff_db_folder = "/staging/leuven/stg_00079/teaching/1000genomes/hg38"

#rules that will run on main job instead of the cluster,generally dont benefit from parallisation
localrules: all, SNP_calling, SNP_norm_filt, snpeff, extract_snps, snp_heatmap

#rule stating what the end goal is, 
#these endpoint will determine which graphs are actually checked for dependecies
#anything not requested will have itself or its dependencies generated
rule all:
    input:
        fastqc_zip=expand("1_fastqc/{sample}_fastqc.zip", sample=sample_names),
        BWA=expand("2_bwa/{sample}.bam", sample=sample_names),
        # summary_png=expand("1_fastqc/{sample}_fastqc/summary.png", sample=sample_names),
        vcf="3_samtools/snps.vcf",
        snpeff = "5_snpeff/snps.annotated.vcf",       
        genes_vcf="genes.vcf",
        heatmap_image="snps_heatmap.png"

# Rule for performing FastQC
rule fastqc:
    input:
        fq = "/staging/leuven/stg_00079/teaching/1000genomes/{sample}.fq.gz"

    output:
        fastqc_zip="1_fastqc/{sample}_fastqc.zip",
        html="1_fastqc/{sample}_fastqc.html",
        summarydata="1_fastqc/{sample}_fastqc/fastqc_data.txt",
        rep1=report("1_fastqc/{sample}_fastqc/Images/per_base_quality.png", category="Fastqc",
                    subcategory="Per base quality", labels={"sample": "{sample}"}),
        rep2=report("1_fastqc/{sample}_fastqc/Images/per_base_sequence_content.png", category="Fastqc",
                     subcategory="Per base sequence content", labels={"sample": "{sample}"}),
        rep3=report("1_fastqc/{sample}_fastqc/summary.txt", category="Fastqc",
                    subcategory="Summary text", labels={"sample": "{sample}"}),
        err=report("1_fastqc/{sample}_fastqc/errors.txt", category="Fastqc",
                    subcategory="Errors", labels={"sample": "{sample}"}),

    shell:
        """
        export PATH=/lustre1/project/stg_00079/teaching/I0U19a_conda_2024/bin:$PATH
    
        echo "Input Fastq: {wildcards.sample} "
        
        #extract the fastqc
        fastqc -o 1_fastqc {input} --extract

        # # Check if it is in FASTQ format
        # first_line=$(head -n 1 {input.fq})
        # third_line=$(sed -n '3p' {input.fq})

        # if ! [[ $first_line == "@"* && $second_line != "+" && $third_line == "+"* ]]; then
        #     echo "File is not in FASTQ format."
        #     false
        # fi

        # #check that file is not truncated
        # if [ $(tail -n 4 {input.fq} | head -n 1 | cut -c1) = "@" ]; then
        #     echo "Fourth last line does not begin with '@'!"
        #     false
        # fi
        
        #these warnings arent severe as bias can be introduced by the adapters used
        #these will be resolved later
        if grep FAIL {output.rep3}; then
            echo "Summary contains Fails!"
        fi

        if grep WARN {output.rep3}; then
            echo "Summary contains warnings!"
        fi
        """


# Rule for performing BWA
rule bwa:
    input:
        fq="/staging/leuven/stg_00079/teaching/1000genomes/{sample}.fq.gz",
    output:
        bam = "2_bwa/{sample}.bam",
        bai = "2_bwa/{sample}.bam.bai",
    params:
        db=BWA_DB_chr21,
    shell:
        """
        export PATH=/lustre1/project/stg_00079/teaching/I0U19a_conda_2024/bin:$PATH

        echo "Performing BWA on: {wildcards.sample} "

        # Check if there are at least 24 files in the params.db beginning with "sequence"
        if [ $(ls -1 {params.db}/sequence* 2>/dev/null | wc -l) -ge 24 ]; then
            echo "Error: Less than 24 files beginning with 'sequence' found."
            false
        fi

        # Check if a file beginning with "cytoBand" exists
        if test -e "${params.db}/cytoBand"*; then
            echo "Error: No file beginning with 'cytoBand' found."
            false
        fi

        # Check if a file beginning with "pwms" exists
        if test -e "${params.db}/pwms"*; then
            echo "Error: No file beginning with 'pwms' found."
            false
        fi

        # Check if a file beginning with "snpEffectPredictor" exists
        if test -e "${params.db}/snpEffectPredictor"*; then
            echo "Error: No file beginning with 'snpEffectPredictor' found."
            false
        fi

        bwa mem {params.db} {input} \
            | samtools sort - \
            > {output.bam}
        samtools index {output.bam}

  
        """      

# Rule for SNP calling using bcftools
rule SNP_calling:
    input:
        db=BWA_DB_chr21,
        bams=expand("2_bwa/{sample}.bam", sample=sample_names),
        
    output:
        vcf="3_samtools/snps.vcf",
    shell:
        """
        echo "Performing SNP calling"
        bcftools mpileup -Ou -f {input.db} {input.bams} \
             | bcftools call -mv -Ov -o {output.vcf}

        #check that the foutput file is not empty
        #snakemake checks files exist but not if they have contents
        if ! [ -s $output_size ]; then
            echo "Error: Output file size is 0kb"
            false
        fi
        """

# Rule for SNP normalization & filtering
rule SNP_norm_filt:
    input:
        db=BWA_DB_chr21,
        vcf="3_samtools/snps.vcf"
    output:
        vcf="4_cleaned/snps.cleaned.vcf"
    shell:
        """
        echo "Normalizing and filtering"
        ( cat {input.vcf} \
           | vt decompose - \
           | vt normalize -n -r {input.db} - \
           | vt uniq - \
           | vt view -f "QUAL>20" -h - \
           > {output.vcf} )

            
        #check that the foutput file is not empty
        if ! [ -s $output_size ]; then
            echo "Error: Output file size is 0kb"
            false
        fi

        #check that ouput is smaller or equal to input
        if ! [ $(stat -c %s {output.vcf}) -ge $(stat -c %s {input.vcf}) ]; then
            echo "Error: Output file size is larger than input file size"
            false
        fi

        """

# Rule for SNP annotation using SNPeff
rule snpeff:
    input:
        vcf = "4_cleaned/snps.cleaned.vcf",
    params:
        snpeff_db_folder = snpeff_db_folder,
        snpeff_jar = snpeff_jar,
        snpeff_genome = snpeff_genome,
    log:
        err="5_snpeff/snakemake.err",
    output:
        vcf = "5_snpeff/snps.annotated.vcf",
        html = "5_snpeff/snpEff_summary.html",
        genetxt = "5_snpeff/snpEff_genes.txt",
    shell:
        """
        echo "Annotating"
        mkdir -p 5_snpeff

        java -Xmx4096m -jar \
            {params.snpeff_jar} eff hg38 \
            -dataDir {params.snpeff_db_folder} \
            {input.vcf} > {output.vcf}

        # move output files to the snpeff output folder
        mv snpEff_genes.txt snpEff_summary.html 5_snpeff

        """

# rule fastqc_report_image:
#     input:
#         summarytxt = "1_fastqc/{sample}_fastqc/summary.txt"
#     output:
#         statuspng = report("1_fastqc/{sample}_fastqc/summary.png",
#                          category='Fastqc',
#                          subcategory='Status',
#                          labels={"sample": "{sample}"})

#     run:
#         import pandas as pd
#         import seaborn as sns
#         import matplotlib.pyplot as plt

#         #load data
#         data = pd.read_csv(input.summarytxt, sep="\t", header=None)
#         data.columns = ['status', 'test', 'sample']

#         #assign dummy x value for scatterplot
#         data['x'] = 1

#         #create image
#         fig = plt.figure(figsize=(4,5))
#         ax = plt.gca()
#         sns.scatterplot(data, x='x', y='test', hue='status', s=200, ax=ax)
#         ax.get_xaxis().set_visible(False)
#         ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
#         plt.tight_layout()
#         plt.title(wildcards.sample)
#         plt.savefig(output.statuspng)

# Rule for extracting SNPs associated with APP, SOD21, and DYRK1A into a new VCF file
rule extract_snps:
    input:
        vcf="5_snpeff/snps.annotated.vcf",
    output:
        genes_vcf="genes.vcf",
        
        # shell:
        #     """
        #     # Extract SNPs associated with APP, SOD21, and DYRK1A from the annotated VCF file
        #     grep -E '^(#|.*\b(APP|SOD21|DYRK1A)\b)' {input.vcf} > {output.genes_vcf}
        #     """
            
        #     # bcftools view -i 'INFO/ANN[*].Gene_Name="APP" || INFO/ANN[*].Gene_Name="SOD21" || INFO/ANN[*].Gene_Name="DYRK1A"' \
        #     #     {input.vcf} -o {output.genes_vcf}    
    
    run:
        import os
        from pathlib import Path
        import sqlite3

        import pandas as pd
        import vcfpy

        dbfile = 'snps.sqlite'
        db = sqlite3.connect(dbfile)

        # Empty lists to store the data in
        snp_records = []
        effect_records = []
        call_records = []

        # These are the columns from a SNPeff record - easy to later assign column names
        effect_rec_names = """snp allele effect impact gene gene_id feature_type feature_id 
                      biotype rank hgvs.c hgvs.p cdna_pos cds_pos prot_pos distance_to_feature
                      messages""".split()

        # start parsing through the VCF:
        j = 0

        #open the vcf iterator:
        reader = vcfpy.Reader.from_path({input.vcf})

        for i, record in enumerate(reader):
            j += 1
            
            # we used vt decompose - so I expect NO multiallelic SNPs
            assert len(record.ALT) == 1 
            
            # this is the ALT allele (first and only)
            alt = record.ALT[0]
            
            # compose a SNP name for joins later on
            snp_name = f"{record.CHROM}:{record.POS}:{record.REF}:{alt.value}"
            
            # Store a SNP record - I collect everything now in a list of dictionaries
            # that I will later convert to a Pandas dataframe
            snp_records.append(
                dict(snp = snp_name,
                    chrom = record.CHROM,
                    pos = record.POS,
                    quality = record.QUAL,
                    ref = record.REF,
                    type = alt.type,
                    alt = alt.value))
            
            
            # Now I will parse through the individual calls
            for call_record in record.calls:
                
                #name of the sample (TLE66_N or TLE66_T)
                sample = os.path.basename(call_record.sample)
                sample = sample.replace('.bam', '')

                # simple genotype - -1 means not called, otherwise it's the number of ALT alleles observer
                # this helps in later queries 
                # Note - this is not perfect - it does ignore uncalled loci (./.)
                genotype_simple = call_record.data['GT'].count('1')
                
                call_records.append(
                    dict(snp = snp_name,
                        sample = sample,
                        genotype = call_record.data['GT'], 
                        genotype_simple = genotype_simple))

            # and parse through all snpEff annotations
            for ann in record.INFO['ANN']:
                ann = ann.split('|')
                # create a dictionary of all fields
                eff_record = dict(zip(effect_rec_names, [snp_name] + ann))
                #from pprint import pprint
                #pprint(eff_record)
                # convert distance to feature to integer
                try:
                    eff_record['distance_to_feature'] = int(eff_record.get(intfield))
                except:
                    eff_record['distance_to_feature'] = -1
                    
                effect_records.append(eff_record)

        #convert lists of dicts to a DataFrame
        snp_records = pd.DataFrame.from_records(snp_records)
        call_records = pd.DataFrame.from_records(call_records)
        effect_records = pd.DataFrame.from_records(effect_records)

        #Save to the database
        print('snp records        :', snp_records.to_sql('snp', db, if_exists='replace', index=False))
        print('snp_call records   :', call_records.to_sql('snp_call', db, if_exists='replace', index=False))
        print('snp_effect records :', effect_records.to_sql('snp_effect', db, if_exists='replace', index=False))

        shell("readlink -f snps.sqlite > {output}")

        pd.read_sql("SELECT * FROM snp LIMIT 5", db)

        # .T transposes - easier viewing
        pd.read_sql("SELECT * FROM snp_effect LIMIT 5", db) 

# Rule for creating an image with a heatmap showing the number of SNPs per individual
rule snp_heatmap:
    input:
        vcf="5_snpeff/snps.annotated.vcf",
    output:
        heatmap_image=report("snps_heatmap.png", category="SNP Heatmap",
                subcategory="Summary text")
    shell:
        """
        bcftools query -l {input.vcf} > samples.txt
        bcftools query -f '%CHROM\t%POS[\t%GT]\n' {input.vcf} | grep -v '#' | cut -f 3- > genotypes.txt
        paste samples.txt genotypes.txt | awk '{print $1,$(NF-1),$NF}' | sort | uniq -c | awk '{print $2,$3,$1}' > snps_per_individual.txt
        awk '{print $1}' snps_per_individual.txt > individuals.txt
        awk '{print $2}' snps_per_individual.txt > snps.txt
        python heatmap.py individuals.txt snps.txt {output.heatmap_image}
        """     