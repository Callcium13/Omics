# this workflow performs
# - FastQC Analysis: Assess the quality of input FASTQ files.
# - BWA Alignment: Align FASTQ reads to the reference genome using BWA.
# - SNP Calling: Identify single nucleotide polymorphisms (SNPs) using bcftools.
# - SNP Normalization and Filtering: Normalize and filter SNP variants.
# - SNP Annotation: Annotate SNPs using SNPeff.
# - SNP Extraction: Extract SNPs associated with specific genes.
# - SNP Heatmap Generation: Generate a heatmap showing the distribution of SNPs among individuals.
# - FastQC Report Image: Create an image summarizing the FastQC results.

#critical errors crash the workflow, noncritical errors such as sequence GC content on fastqc files are echoed

vsc_number = "36251"  # VSC number for filtering id's

# get the list of names from the genomes folder
sample_names, = glob_wildcards("/staging/leuven/stg_00079/teaching/1000genomes/{sample}.fq.gz")
#filter that list based on the last digit
sample_names = [id for id in sample_names if id[6] == vsc_number[-1]]

#Additionally, to lighten the load further, we extracted fastq reads of just chromosome 21 for you to map
BWA_DB_chr21 = '/lustre1/project/stg_00079/teaching/hg38_21/chr21.fa'

#get the jar and data for SNPeff
snpeff_jar = "/staging/leuven/stg_00079/teaching/I0U19a_conda_2024/share/snpeff-5.2-0/snpEff.jar"
snpeff_genome = 'hg38'
snpeff_db_folder = "/staging/leuven/stg_00079/teaching/1000genomes/hg38"

#rules that will run on main job instead of the cluster,generally dont benefit from parallisation
localrules: all, SNP_norm_filt, snpeff, extract_snps, snp_heatmap, fastqc_report_image

#rule stating what the end goal is, 
#these endpoint will determine which graphs are actually checked for dependecies
#anything not requested will have itself or its dependencies generated
rule all:
    input:
        fastqc_zip=expand("1_fastqc/{sample}_fastqc.zip", sample=sample_names),
        BWA=expand("2_bwa/{sample}.bam", sample=sample_names),
        summary_png=expand("1_fastqc/{sample}_fastqc/summary.png", sample=sample_names),
        vcf="3_samtools/snps.vcf",
        snpeff = "5_snpeff/snps.annotated.vcf",       
        genes_vcf="genes.vcf",
        heatmap="snp_heatmap.png"

# Rule for performing FastQC
rule fastqc:
    input:
        fq = "/staging/leuven/stg_00079/teaching/1000genomes/{sample}.fq.gz"

    output:
        fastqc_zip="1_fastqc/{sample}_fastqc.zip",
        html="1_fastqc/{sample}_fastqc.html",
        summarydata="1_fastqc/{sample}_fastqc/fastqc_data.txt",
        rep1=report("1_fastqc/{sample}_fastqc/Images/per_base_quality.png", category="Fastqc",
                    subcategory="Per base quality", labels={"sample": "{sample}"}),
        rep2=report("1_fastqc/{sample}_fastqc/Images/per_base_sequence_content.png", category="Fastqc",
                     subcategory="Per base sequence content", labels={"sample": "{sample}"}),
        rep3=report("1_fastqc/{sample}_fastqc/summary.txt", category="Fastqc",
                    subcategory="Summary text", labels={"sample": "{sample}"})
    resources:
        cpu=1,  # Specify the number of CPU cores needed
    params:
        jobname=lambda wildcards: f"fastqc_{wildcards.sample}",
        outname=lambda wildcards: f"fastqc/{wildcards.sample}",

    shell:
        """
        export PATH=/lustre1/project/stg_00079/teaching/I0U19a_conda_2024/bin:$PATH
    
        echo "Input Fastq: {wildcards.sample} "
        
        #extract the fastqc
        fastqc -o 1_fastqc {input} --extract

        # # Check if it is in FASTQ format
        # first_line=$(head -n 1 {input.fq})
        # third_line=$(sed -n '3p' {input.fq})

        # if ! [[ $first_line == "@"* && $second_line != "+" && $third_line == "+"* ]]; then
        #     echo "File is not in FASTQ format."
        #     false
        # fi

        # #check that file is not truncated
        # if [ $(tail -n 4 {input.fq} | head -n 1 | cut -c1) = "@" ]; then
        #     echo "Fourth last line does not begin with '@'!"
        #     false
        # fi
        
        #these warnings arent severe as bias can be introduced by the adapters used
        # Check for errors and warnings in the summary file
        if grep -q "FAIL" {output.rep3}; then
            echo "Summary contains fails!"
        fi
        if grep -q "WARN" {output.rep3}; then
            echo "Summary contains warnings!"
        fi
        """


# Rule for performing BWA
rule bwa:
    input:
        fq="/staging/leuven/stg_00079/teaching/1000genomes/{sample}.fq.gz",
    output:
        bam = "2_bwa/{sample}.bam",
        bai = "2_bwa/{sample}.bam.bai",
    resources:
        cpu=1,  # Specify the number of CPU cores needed
    params:
        db=BWA_DB_chr21,
        jobname=lambda wildcards: f"bwa_{wildcards.sample}",
        outname=lambda wildcards: f"bwa/{wildcards.sample}",
    shell:
        """
        export PATH=/lustre1/project/stg_00079/teaching/I0U19a_conda_2024/bin:$PATH

        echo "Performing BWA on: {wildcards.sample} "

        # Check if there are at least 24 files in the params.db beginning with "sequence"
        if [ $(ls -1 {params.db}/sequence* 2>/dev/null | wc -l) -ge 24 ]; then
            echo "Error: Less than 24 files beginning with 'sequence' found."
            false
        fi

        # Check if a file beginning with "cytoBand" exists
        if test -e "${params.db}/cytoBand"*; then
            echo "Error: No file beginning with 'cytoBand' found."
            false
        fi

        # Check if a file beginning with "pwms" exists
        if test -e "${params.db}/pwms"*; then
            echo "Error: No file beginning with 'pwms' found."
            false
        fi

        # Check if a file beginning with "snpEffectPredictor" exists
        if test -e "${params.db}/snpEffectPredictor"*; then
            echo "Error: No file beginning with 'snpEffectPredictor' found."
            false
        fi

        bwa mem {params.db} {input} \
            | samtools sort - \
            > {output.bam}
        samtools index {output.bam}

  
        """      

# Rule for SNP calling using bcftools
rule SNP_calling:
    input:
        db=BWA_DB_chr21,
        bams=expand("2_bwa/{sample}.bam", sample=sample_names),
        
    output:
        vcf="3_samtools/snps.vcf",

    resources:
        cpu=4,  # number of cpu's

    params:
        jobname = "SNP_calling",
        outname = "SNP_calling",

    shell:
        """
        export PATH=/lustre1/project/stg_00079/teaching/I0U19a_conda_2024/bin:$PATH
        echo "Performing SNP calling"
        bcftools mpileup -Ou -f {input.db} {input.bams} \
            | bcftools call -mv -Ov -o {output.vcf} --threads {resources.cpu}

        #check that the foutput file is not empty
        #snakemake checks files exist but not if they have contents
        if ! [ -s {output.vcf} ]; then
            echo "Error: Output file size is 0kb"
            false
        fi
        """

# Rule for SNP normalization & filtering
rule SNP_norm_filt:
    input:
        db=BWA_DB_chr21,
        vcf="3_samtools/snps.vcf"
    output:
        vcf="4_cleaned/snps.cleaned.vcf"
    shell:
        """
        echo "Normalizing and filtering"
        ( cat {input.vcf} \
           | vt decompose - \
           | vt normalize -n -r {input.db} - \
           | vt uniq - \
           | vt view -f "QUAL>20" -h - \
           > {output.vcf} )

            
        #check that the foutput file is not empty
        if ! [ -s {output.vcf} ]; then
            echo "Error: Output file size is 0kb"
            false
        fi

        #check that ouput is smaller or equal to input
        if ! [ $(stat -c %s {output.vcf}) -le $(stat -c %s {input.vcf}) ]; then
            echo "Error: Output file size is larger than input file size"
            false
        fi

        """

# Rule for SNP annotation using SNPeff
rule snpeff:
    input:
        vcf = "4_cleaned/snps.cleaned.vcf",
    params:
        snpeff_db_folder = snpeff_db_folder,
        snpeff_jar = snpeff_jar,
        snpeff_genome = snpeff_genome,
    log:
        err="5_snpeff/snakemake.err",
    output:
        vcf = "5_snpeff/snps.annotated.vcf",
        html = "5_snpeff/snpEff_summary.html",
        genetxt = "5_snpeff/snpEff_genes.txt",
    shell:
        """
        echo "Annotating"
        mkdir -p 5_snpeff

        java -Xmx4096m -jar \
            {params.snpeff_jar} eff hg38 \
            -dataDir {params.snpeff_db_folder} \
            {input.vcf} > {output.vcf}

        # move output files to the snpeff output folder
        mv snpEff_genes.txt snpEff_summary.html 5_snpeff

        """



# Rule for extracting SNPs associated with APP, SOD21, and DYRK1A into a new VCF file
rule extract_snps:
    input:
        vcf="5_snpeff/snps.annotated.vcf",
    output:
        genes_vcf="genes.vcf",

    shell:
        """
        bcftools view -i 'INFO/ANN ~ "|APP-" | INFO/ANN ~ "|SOD21-" | INFO/ANN ~ "|DYRK1A-" | INFO/ANN ~ "-APP|" | INFO/ANN ~ "-SOD21|" | INFO/ANN ~ "-DYRK1A|"' {input.vcf} > {output.genes_vcf}
        
        # Check if there are more lines than just the header in the output VCF file
        num_lines=$(wc -l < {output.genes_vcf})
        if [ "$num_lines" -le 35 ]; then
            echo "Error: Number of lines in the output VCF file is not greater than 35"
            false
        fi
        
        """

# # Rule for creating an image with a heatmap showing the number of SNPs per individual
# rule snp_heatmap:
#     input:
#         vcf="genes.vcf",
#     output:
#         heatmap="snp_heatmap.png",  # Output image path
#         report("snp_heatmap.png", category='SNP Analysis', subcategory='Heatmap')  # Report for the heatmap
#     run:
#         import pandas as pd
#         import seaborn as sns
#         import matplotlib.pyplot as plt

#         # Load SNP data
#         snp_df = pd.read_csv(input.vcf, sep="\t", skiprows=2)  # Skip the first two rows which are header lines

#         # Extract relevant columns for SNP counts
#         snp_counts = snp_df.iloc[:, 9:]  # Assuming SNP counts start from 10th column (index 9)

#         # Add 'Sample' column to SNP counts dataframe
#         sample_names = snp_df.columns[9:]  # Assuming sample names start from 10th column
#         snp_counts['Sample'] = sample_names

#         # Melt the dataframe to have 'Sample', 'Gene', and 'SNP_Count' columns
#         snp_counts_melted = snp_counts.melt(id_vars='Sample', var_name='Gene', value_name='SNP_Count')

#         # Pivot the data to create a matrix with samples as rows, genes as columns, and SNP counts as values
#         snp_matrix = snp_counts_melted.pivot(index='Sample', columns='Gene', values='SNP_Count')

#         # Create the heatmap
#         plt.figure(figsize=(8, 6))  # Adjust the figure size as needed
#         sns.heatmap(snp_matrix, cmap="YlGnBu", annot=True, fmt="d")  # Adjust the color map and annotation format as needed
#         plt.title("SNP Counts per Gene per Individual")
#         plt.xlabel("Gene")
#         plt.ylabel("Individual")
#         plt.savefig(output.heatmap)  # Save the heatmap image

#generate some images for the report
rule fastqc_report_image:
    input:
        summarytxt = "1_fastqc/{sample}_fastqc/summary.txt"
    output:
        statuspng = report("1_fastqc/{sample}_fastqc/summary.png",
                         category='Fastqc',
                         subcategory='Status',
                         labels={"sample": "{sample}"})

    run:
        import pandas as pd
        import seaborn as sns
        import matplotlib.pyplot as plt

        #load data
        data = pd.read_csv(input.summarytxt, sep="\t", header=None)
        data.columns = ['status', 'test', 'sample']

        #assign dummy x value for scatterplot
        data['x'] = 1

        #create image
        fig = plt.figure(figsize=(4,5))
        ax = plt.gca()
        sns.scatterplot(data, x='x', y='test', hue='status', s=200, ax=ax)
        ax.get_xaxis().set_visible(False)
        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
        plt.tight_layout()
        plt.title(wildcards.sample)
        plt.savefig(output.statuspng)